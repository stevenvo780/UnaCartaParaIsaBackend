services:
  backend-gpu:
    build:
      context: .
      dockerfile: Dockerfile.gpu
      network: host
    ports:
      - "8080:8080"
      - "9229:9229"  # Node.js inspector
    dns:
      - 8.8.8.8
      - 8.8.4.4
    volumes:
      # Montar código para desarrollo (hot reload)
      - ./src:/app/src
      - ./saves:/app/saves
      # No montar node_modules para usar los del contenedor
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    environment:
      - NODE_ENV=development
      - TF_FORCE_GPU_ALLOW_GROWTH=true
      - TF_CPP_MIN_LOG_LEVEL=2
      - CUDA_VISIBLE_DEVICES=0
      # Limitar threads de TensorFlow para evitar saturación de CPU
      - TF_NUM_INTEROP_THREADS=4
      - TF_NUM_INTRAOP_THREADS=4
      - OMP_NUM_THREADS=4
      # Evitar spinning de threads CUDA - operaciones síncronas
      - CUDA_LAUNCH_BLOCKING=1
      # Hacer que threads de CUDA esperen pasivamente en lugar de spin-wait
      - CUDA_DEVICE_SCHEDULE_BLOCKING_SYNC=1
      # CRÍTICO: Usa threads privados para GPU, evita spinning/polling en CPU
      - TF_GPU_THREAD_MODE=gpu_private
      - TF_GPU_THREAD_COUNT=2
      # Evita que CUDA haga spin-wait, usa blocking sync
      # Limitar threads de libuv (UV_THREADPOOL_SIZE por defecto es 4)
      - UV_THREADPOOL_SIZE=2
      - CUDA_DEVICE_SCHEDULE=BLOCKING_SYNC
      # Pool elástico de chunk workers - escala bajo demanda
      # MIN: Workers siempre calientes (arranque rápido)
      # MAX: Límite superior para evitar saturación
      # IDLE_TIMEOUT: Segundos antes de matar workers ociosos
      - CHUNK_WORKERS_MIN=1
      - CHUNK_WORKERS_MAX=8
      - CHUNK_WORKER_IDLE_TIMEOUT_MS=30000
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/api/sim/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - default
      - uci-monitoring

networks:
  uci-monitoring:
    external: true
